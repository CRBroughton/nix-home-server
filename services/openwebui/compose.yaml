# Open WebUI + Ollama (CPU only) - LLM chat interface
# Run with: podman compose up -d

services:
  tailscale-openwebui:
    image: tailscale/tailscale:latest
    container_name: tailscale-openwebui
    hostname: openwebui
    restart: unless-stopped
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SERVE_CONFIG=/config/serve.json
    volumes:
      - ./tailscale:/var/lib/tailscale:Z
      - ./serve.json:/config/serve.json:ro,Z
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    devices:
      - /dev/net/tun:/dev/net/tun

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    network_mode: service:tailscale-openwebui
    depends_on:
      - tailscale-openwebui
    volumes:
      - ollama-data:/root/.ollama:Z
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 512M

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    network_mode: service:tailscale-openwebui
    depends_on:
      - tailscale-openwebui
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://127.0.0.1:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
    volumes:
      - openwebui-data:/app/backend/data:Z
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 256M

volumes:
  ollama-data:
  openwebui-data: